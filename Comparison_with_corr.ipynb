{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Comparison_with_corr.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "12x0eBp1k9V8xsfE56iwKVU0D6w5WC97k",
      "authorship_tag": "ABX9TyOH6xfAdZphklsvQrxBBEz0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzKtPhy6Fpy9",
        "colab_type": "code",
        "outputId": "db074ede-cc3f-4f71-fb6f-fcbe7e2258ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 908
        }
      },
      "source": [
        "%pip install bayesian-optimization\n",
        "%pip install livelossplot\n",
        "%pip install lightgbm\n",
        "%pip install shap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pylab as pl\n",
        "import numpy as np\n",
        "import os, contextlib, sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import explained_variance_score, r2_score\n",
        "from sklearn.preprocessing import quantile_transform, StandardScaler, RobustScaler\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from livelossplot import PlotLosses\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import datetime\n",
        "import shap\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from itertools import combinations\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "import os, contextlib, sys\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn import decomposition\n",
        "from sklearn import datasets\n",
        "\n",
        "\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks\n",
        "\n",
        "np.random.seed(seed=1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.15.1)\n",
            "Requirement already satisfied: livelossplot in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: bokeh; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from livelossplot) (1.4.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from livelossplot) (5.5.0)\n",
            "Requirement already satisfied: matplotlib; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from livelossplot) (3.2.1)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (2.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (7.0.0)\n",
            "Requirement already satisfied: tornado>=4.3 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (4.5.3)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (3.13)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (1.18.5)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (20.4)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->livelossplot) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->livelossplot) (4.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->livelossplot) (47.1.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->livelossplot) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->livelossplot) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->livelossplot) (2.1.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->livelossplot) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->livelossplot) (4.8.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (2.4.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh; python_version >= \"3.6\"->livelossplot) (1.1.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->livelossplot) (0.2.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->livelossplot) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->livelossplot) (0.6.0)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->lightgbm) (0.15.1)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.6/dist-packages (0.35.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from shap) (1.0.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from shap) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from shap) (1.18.5)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.6/dist-packages (from shap) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from shap) (0.22.2.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->shap) (0.15.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->shap) (1.12.0)\n",
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSWV_IuMNqDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpu = torch.device(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9PShidcOFOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loaddata(directory, t=True):\n",
        "  \n",
        "    feature_list = list()\n",
        "    train_data_list = list()\n",
        "    train_target_list = list()\n",
        "    name = list()\n",
        "\n",
        "\n",
        "    for i,filename in enumerate(os.listdir(directory)):\n",
        "        data_matrix = np.genfromtxt(f'{directory}/{filename}')\n",
        "        if filename == 'Data2scaled.txt': \n",
        "            name.append(filename)\n",
        "            raw_data = np.genfromtxt('data/etalon_etalon_29May20.ccfSum-telemetry.csv',delimiter=',',names=True)\n",
        "            feature_list.append(list(raw_data.dtype.names))\n",
        "            data_matrix = np.delete(data_matrix, 2, 1)\n",
        "            test_data = data_matrix[:,[i for i in range(len(data_matrix[1])) if i!=1]]\n",
        "            test_target = data_matrix[:,1]\n",
        "        else:\n",
        "            data_matrix = np.delete(data_matrix, 2, 1)\n",
        "            train_data_list.append(data_matrix[:,[i for i in range(len(data_matrix[1])) if i!=1]])\n",
        "            train_target_list.append(data_matrix[:,1])\n",
        "    feature_list = feature_list[0]\n",
        "    feature_list.remove('RV')\n",
        "    feature_list.remove('RV_ERR')    \n",
        "\n",
        "\n",
        "    if t == True:\n",
        "      train_data_list = [quantile_transform(data,n_quantiles=100,copy=True) for data in train_data_list]\n",
        "      test_data = quantile_transform(test_data,n_quantiles=100,copy=True)\n",
        "      train_data_torch = [torch.from_numpy(data).unsqueeze(dim=1).float().to(gpu) for data in train_data_list]\n",
        "      train_target_torch = [torch.from_numpy(data).unsqueeze(dim=1).float().to(gpu) for data in train_target_list]\n",
        "      test_data_torch, test_target_torch = torch.from_numpy(np.array(test_data)).unsqueeze(dim=1).float().to(gpu), torch.from_numpy(np.array(test_target)).float().to(gpu)\n",
        "      return train_data_torch, train_target_torch, test_data_torch, test_target_torch, np.array(feature_list,dtype=str)\n",
        "\n",
        "    \n",
        "    elif t == False:\n",
        "      train_data_list = [quantile_transform(data,n_quantiles=100,copy=True) for data in train_data_list]\n",
        "      test_data = quantile_transform(test_data,n_quantiles=100,copy=True)\n",
        "      train_data = np.concatenate(train_data_list, axis=0)\n",
        "      train_target = np.concatenate(train_target_list, axis=0)\n",
        "      train_data = np.delete(train_data, 0, 1)\n",
        "      test_data = np.delete(test_data, 0, 1)\n",
        "      return train_data, train_target, test_data, test_target, name[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29po0sRCdqWK",
        "colab_type": "text"
      },
      "source": [
        "**Loading in data:** For gpu (torch-based, t=True) and cpu (sklearn-based, t=False)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0PvAGEGOkih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_torch, train_target_torch, test_data_torch, test_target_torch, feature_list = loaddata('Scaleddata', t=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQqn7JXe2MXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_numpy, train_target_numpy, test_data_numpy, test_target_numpy, name = loaddata('Scaleddata', t=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SpkwRMu6ESC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, inputsize, layers, hiddensize, drop_out):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.inputsize = inputsize\n",
        "        self.hiddensize = hiddensize\n",
        "        self.layers = layers\n",
        "        self.drop_out = drop_out\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=self.inputsize, hidden_size = self.hiddensize, num_layers=layers, dropout=drop_out)\n",
        "        self.hidden2radial = nn.Linear(in_features=hiddensize, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        #x = F.elu(x)\n",
        "        x = self.hidden2radial(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTo46Fq2dNqe",
        "colab_type": "text"
      },
      "source": [
        "**Bayesian Optimization:** A generalized version to optimize both torch based and sklearn based algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3bCGJxUHhIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def val(inputsize: int, algorithm, data, targets, **params):\n",
        "    param_list = list()\n",
        "    for arg in params.values():\n",
        "      param_list.append(arg)\n",
        "\n",
        "    if algorithm == LSTMTagger:\n",
        "      estimator = algorithm(inputsize, int(param_list[2]), int(param_list[1]), param_list[0])\n",
        "      optimizer = torch.optim.Adam(estimator.parameters(),lr=param_list[3])\n",
        "      judge = list()\n",
        "      estimator = estimator.to(gpu)\n",
        "      criterion = nn.MSELoss()\n",
        "      for i,valdata in enumerate(train_data_torch):\n",
        "          traindata = train_data_torch[:i] + train_data_torch[i+1:]\n",
        "          traintarget = train_target_torch[:i] + train_target_torch[i+1:]\n",
        "          valtarget = train_target_torch[i]\n",
        "          judge_list = list()\n",
        "          n_epochs = 100\n",
        "          for e in range(n_epochs):\n",
        "            estimator.train()\n",
        "            epoch_losses = list()\n",
        "            epoch_evs = list()\n",
        "            acc_list = list()\n",
        "            loss_list = list()\n",
        "            for batch in range(len(traindata)):\n",
        "              estimator.zero_grad()\n",
        "              optimizer.zero_grad() \n",
        "              prediction = estimator(traindata[batch])\n",
        "              target = traintarget[batch]\n",
        "              # Calculating the loss function\n",
        "              loss = criterion(prediction.squeeze(dim=2), target)\n",
        "              # Calculating the gradient\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "            with torch.no_grad():\n",
        "              estimator.eval()\n",
        "              train_prediction = estimator(valdata).squeeze(dim=1)\n",
        "              loss_list.append( float(criterion((train_prediction),valtarget).detach().cpu()) )\n",
        "              acc_list.append( explained_variance_score(valtarget.cpu(), train_prediction.cpu()) )\n",
        "            judge_list.append(np.mean(acc_list))\n",
        "      return np.mean(judge_list)\n",
        "\n",
        "    else:\n",
        "      for key in params.keys():\n",
        "        if params[key] >= 1:\n",
        "          params[key] = int(params[key])\n",
        "      estimator = algorithm(random_state=27, **params)\n",
        "      cval = cross_val_score(estimator, data, targets, cv=5,scoring='explained_variance')\n",
        "      return cval.mean()\n",
        "\n",
        "def optimize(algorithm, data, targets, params, n_iter):\n",
        "    def crossval_wrapper(data=data, targets=targets, **params):\n",
        "        return val(inputsize = 26,\n",
        "                   algorithm = algorithm, \n",
        "                   data = data, \n",
        "                   targets = targets, \n",
        "                   **params)\n",
        "\n",
        "    optimizer = BayesianOptimization(f=crossval_wrapper, \n",
        "                                     pbounds=params, \n",
        "                                     random_state=27, \n",
        "                                     verbose=2)\n",
        "    optimizer.maximize(init_points=2, n_iter=n_iter)\n",
        "\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrLZo9b3nlvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_list = list()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVsm6am4dHyV",
        "colab_type": "text"
      },
      "source": [
        "**Long short term memory:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVWYj3SF6I6y",
        "colab_type": "code",
        "outputId": "a3c41501-0511-403c-e484-4ae8fc697652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "params_LSTM = {\"layers\": (1, 3),\n",
        "               \"hiddensize\": (100, 400),\n",
        "               \"drop_out\": (0.2, 0.6),\n",
        "               \"lr\": (0.0001, 0.00001)}\n",
        "\n",
        "BO_LSTM = optimize(algorithm = LSTMTagger, data = train_data_torch, targets = train_target_torch, params = params_LSTM, n_iter = 6)\n",
        "\n",
        "print(BO_LSTM.max)\n",
        "\n",
        "max_params_LSTM = BO_LSTM.max['params']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | drop_out  | hidden... |  layers   |    lr     |\n",
            "-------------------------------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7485  \u001b[0m | \u001b[0m 0.3703  \u001b[0m | \u001b[0m 344.4   \u001b[0m | \u001b[0m 2.471   \u001b[0m | \u001b[0m 2.188e-0\u001b[0m |\n",
            "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.8797  \u001b[0m | \u001b[95m 0.3534  \u001b[0m | \u001b[95m 393.8   \u001b[0m | \u001b[95m 2.786   \u001b[0m | \u001b[95m 8.113e-0\u001b[0m |\n",
            "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.5813  \u001b[0m | \u001b[0m 0.2306  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 2.132   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4854891532706884 and num_layers=1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.7117  \u001b[0m | \u001b[0m 0.4855  \u001b[0m | \u001b[0m 400.0   \u001b[0m | \u001b[0m 1.257   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7MiK4ZDGvwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.MSELoss()\n",
        "judge = list()\n",
        "for i,valdata in enumerate(train_data_torch):\n",
        "      model = LSTMTagger(26,layers=int(max_params_LSTM['layers']), hiddensize=int(max_params_LSTM['hiddensize']),drop_out=max_params_LSTM['drop_out']).to(gpu)\n",
        "      optimizer = optim.Adam(model.parameters(),lr=max_params_LSTM['lr'])\n",
        "\n",
        "      traindata = train_data_torch[:i] + train_data_torch[i+1:]\n",
        "      traintarget = train_target_torch[:i] + train_target_torch[i+1:]\n",
        "      valtarget = train_target_torch[i]\n",
        "      judge_list = list()\n",
        "\n",
        "      plotlosses = PlotLosses()\n",
        "      n_epochs = 100\n",
        "      for e in range(n_epochs):\n",
        "          model.train()\n",
        "          epoch_losses = list()\n",
        "          epoch_evs = list()\n",
        "          acc_list = list()\n",
        "          loss_list = list()\n",
        "          for batch in range(len(traindata)):\n",
        "              model.zero_grad()\n",
        "              optimizer.zero_grad() \n",
        "              prediction = model(traindata[batch])\n",
        "              target = traintarget[batch]\n",
        "              # Calculating the loss function\n",
        "              loss = criterion(prediction.squeeze(dim=2), target)\n",
        "              epoch_losses.append(float(loss))\n",
        "              evs = explained_variance_score(target.squeeze(dim=1).detach().cpu().numpy(),prediction.squeeze(dim=1).detach().cpu().numpy())\n",
        "              epoch_evs.append(evs)\n",
        "              # Calculating the gradient\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "          with torch.no_grad():\n",
        "              model.eval()\n",
        "              train_prediction = model(valdata).squeeze(dim=1)\n",
        "              loss_list.append( float(criterion((train_prediction),valtarget).detach().cpu()) )\n",
        "              acc_list.append( explained_variance_score(valtarget.cpu(), train_prediction.cpu()) )\n",
        "          print(e, np.mean(epoch_losses), np.mean(epoch_evs))\n",
        "          judge_list.append(np.mean(acc_list))\n",
        "      judge.append([np.mean(judge_list),model])\n",
        "sorted(judge, key=lambda x: x[0])\n",
        "winner = judge[0]\n",
        "model_list.append(winner[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIllPyl9dCHu",
        "colab_type": "text"
      },
      "source": [
        "**Light GBM:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ljVMPa9F0BZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params_LGBM = {'num_leaves': (2, 3),\n",
        "               'max_depth': (70, 200),\n",
        "               'learning_rate': (0.00001, 0.01)}\n",
        "\n",
        "BO_LGBM = optimize(algorithm = lgb.LGBMRegressor, data = train_data_numpy, targets = train_target_numpy, params = params_LGBM, n_iter=30)\n",
        "\n",
        "print(BO_LGBM.max)\n",
        "\n",
        "max_params_LGBM = BO_LGBM.max['params']\n",
        "\n",
        "model_LGBM = lgb.LGBMRegressor(num_leaves = int(max_params_LGBM['num_leaves']), \n",
        "                               max_depth= int(max_params_LGBM['max_depth']), \n",
        "                               learning_rate = max_params_LGBM['learning_rate']\n",
        "                               )\n",
        "\n",
        "\n",
        "model_LGBM.fit(train_data_numpy, train_target_numpy)\n",
        "\n",
        "\n",
        "\n",
        "model_list.append(model_LGBM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oag1LLnWc5bs",
        "colab_type": "text"
      },
      "source": [
        "**XG Boost:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXTtPVSyu4sm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params_XGB = {\"eta\": (0.1, 0.5),\n",
        "               \"max_depth\": (1, 10),\n",
        "               \"num_round\": (1, 40),\n",
        "              \"n_estimators\": (2, 10)}\n",
        "\n",
        "BO_XGB = optimize(algorithm = xgb.XGBRegressor, data = train_data_numpy, targets = train_target_numpy, params = params_XGB, n_iter=30)\n",
        "\n",
        "print(BO_XGB.max)\n",
        "\n",
        "max_params_XGB = BO_XGB.max['params']\n",
        "\n",
        "model_XGB = xgb.XGBRegressor(max_depth = int(max_params_XGB['max_depth']), \n",
        "                             num_round = int(max_params_XGB['num_round']),\n",
        "                             eta = max_params_XGB['eta'],\n",
        "                             n_estimators = int(max_params_XGB['n_estimators'])\n",
        "                             )     \n",
        "     \n",
        "\n",
        "model_XGB.fit(train_data_numpy, train_target_numpy)     \n",
        "\n",
        "model_list.append(model_XGB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vXlbl5le3cX",
        "colab_type": "text"
      },
      "source": [
        "**Ordinary Least Squares:** No need for Bayesian Optimization \\\\\n",
        "**Multi-Layer Perception:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcYiHBPND68R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params_MLP = {\"hidden_layer_sizes\": (50, 300),\n",
        "               \"alpha\": (0, 0.5),\n",
        "               \"learning_rate_init\": (0.0001, 0.5),}\n",
        "\n",
        "BO_MLP = optimize(algorithm = MLPRegressor, data = train_data_numpy, targets = train_target_numpy, params = params_MLP, n_iter=30)\n",
        "\n",
        "print(BO_MLP.max)\n",
        "\n",
        "max_params_MLP = BO_MLP.max['params']\n",
        "\n",
        "model_MLP = MLPRegressor(hidden_layer_sizes= int(max_params_MLP['hidden_layer_sizes']),\n",
        "                         alpha = max_params_MLP['alpha'],\n",
        "                         learning_rate_init= max_params_MLP['learning_rate_init'],\n",
        "                         )\n",
        "model_MLP.fit(train_data_numpy, train_target_numpy)\n",
        "\n",
        "model_list.append(model_MLP)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMX4wqpqVRJI",
        "colab_type": "text"
      },
      "source": [
        "**Ordinary Least Squares:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMcHvFwMT3Ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_OLS = linear_model.LinearRegression()\n",
        "model_OLS.fit(train_data_numpy, train_target_numpy)\n",
        "model_list.append(model_OLS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC6vHDdOVkBA",
        "colab_type": "text"
      },
      "source": [
        "**Plot:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM9hmafAhHVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "time = test_data_torch.cpu().squeeze().detach().numpy()[:,0]\n",
        "\n",
        "fig = plt.figure('All models')\n",
        "colorlist = ['#1b9e77', '#d95f02', '#7570b3', '#e7298a', '#66a61e', '#e6ab02']\n",
        "\n",
        "\n",
        "plt.plot(time, test_target_numpy, linestyle='-', linewidth=4, label='True', color='k')\n",
        "for i,model in enumerate(model_list):\n",
        "  if i == 0:\n",
        "    prediction = model(test_data_torch).squeeze(dim=1).detach().cpu().numpy()\n",
        "    plt.plot(time, prediction, linestyle='--',label=f'{model}'.split('(')[0].replace('Tagger',''), color=colorlist[i],linewidth=2)\n",
        "  elif i == len(model_list)-1:\n",
        "    plt.plot(time, model.predict(test_data_numpy),linestyle='--',label=f'{model}'.split('(')[0].replace('Regression',''), color=colorlist[i], linewidth=2)\n",
        "  else:\n",
        "    plt.plot(time, model.predict(test_data_numpy),linestyle='--',label=f'{model}'.split('(')[0].replace('Regressor',''), color=colorlist[i], linewidth=2)\n",
        "plt.legend(loc='best')\n",
        "plt.title(f\"Testing on {name.replace('scaled.txt','')}\")\n",
        "plt.xlabel('Scaled time')\n",
        "plt.ylabel('Scaled drift')\n",
        "plt.style.use('seaborn-white')\n",
        "plt.style.use('seaborn-ticks')\n",
        "fig.patch.set_facecolor('#f2f2f2')\n",
        "cur_time = str(datetime.datetime.now())\n",
        "cur_time = cur_time.split('.')[0]\n",
        "cur_time = cur_time.replace(' ','')\n",
        "cur_time = cur_time.replace('-','')\n",
        "cur_time = cur_time.replace(':','')\n",
        "\n",
        "fig.savefig(f\"figs/TotalTest{name.replace('scaled.txt','')}_{cur_time}.png\", facecolor=fig.get_facecolor(), format='png', dpi=1200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ-7kBWJnHFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "for i,model in enumerate(model_list):\n",
        "  filename = f'models/finalized_model_{i}.sav'\n",
        "  pickle.dump(model, open(filename, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywSe4IbvP0ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_model_list = list()\n",
        "filename = f'models/finalized_model_{i}.sav'\n",
        "for i,filename in enumerate(os.listdir('models')):\n",
        "  new_model_list.append(pickle.load(open(f\"models/{filename}\", 'rb')))\n",
        "\n",
        "\n",
        "\n",
        "all_data = np.vstack([train_data_numpy, test_data_numpy])\n",
        "feature_list = np.delete(feature_list, np.argwhere( feature_list == 'JD_UTC'),0)\n",
        "\n",
        "for i, model in enumerate(new_model_list):\n",
        "  if i == 0:\n",
        "    print('Not on SHAP :(')\n",
        "    continue\n",
        "    #for k in range(len(train_data_torch)):\n",
        "      #explainer = shap.GradientExplainer(model, data= train_data_torch[k])\n",
        "      #modelname = f'{model}'.split('(')[0].replace('Regressor','')\n",
        "      #modelname = f'{modelname}_{k}'\n",
        "      #model.train()\n",
        "      #shap_values = explainer.shap_values(train_data_torch[k])\n",
        "      #shap.summary_plot(shap_values, train_data_torch[k], \n",
        "      #                plot_type=\"bar\",show=False, feature_names=feature_list,\n",
        "      #                color=\"#4d9221\")\n",
        "    \n",
        "  elif i == len(model_list)-1:\n",
        "    explainer = shap.LinearExplainer(model, data = all_data)\n",
        "    modelname = f'{model}'.split('(')[0].replace('Regression','')\n",
        "    shap_values = explainer.shap_values(all_data)\n",
        "    shap.summary_plot(shap_values, all_data, plot_type = \"bar\", \n",
        "                      show=False, feature_names=feature_list,\n",
        "                      color=\"#e7298a\")\n",
        "  elif i == 3:\n",
        "    print('Not on SHAP :(')\n",
        "    continue\n",
        "  else:\n",
        "    print(model, '\\n')\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    modelname = f'{model}'.split('(')[0].replace('Regressor','')\n",
        "    shap_values = explainer.shap_values(all_data)\n",
        "    shap.summary_plot(shap_values, all_data, plot_type = \"bar\", \n",
        "                      show=False, feature_names=feature_list,\n",
        "                      color=\"#e7298a\")\n",
        "  path = f\"figs/SHAP{name.replace('scaled.txt','')}_{modelname}_{cur_time}.png\"\n",
        "  pl.figure(figsize=(10,50))\n",
        "  pl.tight_layout()\n",
        "  #pl.xlabel('Scaled time')\n",
        "  #pl.ylabel('Scaled drift')\n",
        "  pl.style.use('seaborn-white')\n",
        "  pl.style.use('seaborn-ticks')\n",
        "\n",
        "  pl.savefig(path, format='png', dpi=1200, facecolor=fig.get_facecolor())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltvcTIVOuCrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(shap_values.mean(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2bxsYHvjzOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  #Taken from https://zhiyzuo.github.io/Pearson-Correlation-CI-in-Python/\n",
        "\n",
        "#plt.rcParams.update({'font.size': 4})\n",
        "\n",
        "def pearsonr_ci(x, y, alpha=0.05, _print=True):\n",
        "\t''' calculate Pearson correlation along with the confidence interval using scipy and numpy\n",
        "\tParameters\n",
        "\t----------\n",
        "\tx, y : iterable object such as a list or np.array\n",
        "\tInput for correlation calculation\n",
        "\talpha : float\n",
        "\tSignificance level. 0.05 by default\n",
        "\tReturns\n",
        "\t-------\n",
        "\tr : float\n",
        "\tPearson's correlation coefficient\n",
        "\tpval : float\n",
        "\tThe corresponding p value\n",
        "\tlo, hi : float\n",
        "\tThe lower and upper bound of confidence intervals\n",
        "\t'''\n",
        "\n",
        "\tr, p = stats.pearsonr(x,y)\n",
        "\tr_z = np.arctanh(r)\n",
        "\tse = 1/np.sqrt(x.size-3)\n",
        "\tz = stats.norm.ppf(1-alpha/2)\n",
        "\tlo_z, hi_z = r_z-z*se, r_z+z*se\n",
        "\tlo, hi = np.tanh((lo_z, hi_z))\n",
        "\tif _print: print(f\"\\t corr {r:.3f} in [{lo:.3f}, {hi:.3f}], with p={p:.3f}\")\n",
        "\treturn lo, hi, p\n",
        "\n",
        "def corrfunc(x, y, **kws):\n",
        "\tr, _ = stats.pearsonr(x, y)\n",
        "\tax = plt.gca()\n",
        "\tif not np.isnan(r): ax.annotate(\"r = {:.2f} € [{:.2f},{:.2f}], p = {:.2f} \".format(r, *pearsonr_ci(x,y,_print=False)), xy=(.1, .9), xycoords=ax.transAxes)\n",
        "\n",
        "def meanfunc(x, **kws):\n",
        "\tm, s = x.mean(), x.std()\n",
        "\tax = plt.gca()\n",
        "\tax.annotate(f\"{m:.2f}±{s:.2f}\", xy=(.1, .9), xycoords=ax.transAxes)\n",
        "\n",
        "def summary_analyze(data: pd.DataFrame, filename):\n",
        "\tprint('shape', data.shape)\n",
        "\t# Mean and std...\n",
        "\tprint(data.describe())\n",
        "\n",
        "\t# Covariance matrix\n",
        "\tfor pair in combinations(data.columns, r=2):\n",
        "\t\tprint(f\"{pair[0]} and {pair[1]}\")\n",
        "\t\tpearsonr_ci(data[pair[0]], data[pair[1]])\n",
        "\t# print(data.corr())\n",
        "\t# plt.matshow(data.corr())\n",
        "\t# plt.show()\n",
        "\n",
        "\t# Joint and marginal\n",
        "\tplt.figure(figsize=(50,50))\n",
        "\tcross_values = ['FIES_INSIDE_BLACK_BOX_AT_REAR',\n",
        "\t\t\t\t\t'FIES_INSIDE_BLACK_BOX_CENTRE',\n",
        "\t\t\t\t\t'FIES_ABOVE_HEATER_RADIATOR',\n",
        "\t\t\t\t\t'FIES_GRATING_TANK_HOUSING',\n",
        "\t\t\t\t\t'FIES_INSIDE_WHITE_BOX_REAR',\n",
        "\t\t\t\t\t'FIES_INSIDE_WHITE_BOX_CENTRE',\n",
        "\t\t\t\t\t'FIES_FRONT_ROOM']\n",
        "\tg = sb.PairGrid(data, palette=[\"red\"],x_vars=cross_values, y_vars=cross_values)\n",
        "\tg.map_diag(sb.distplot, kde=False, bins=10, color='#4d9221')\n",
        "\tg.map_diag(meanfunc)\n",
        "\tg.map_upper(sb.kdeplot, cmap=\"Greens_d\")\n",
        "\n",
        "\tg.map_lower(plt.scatter, s=10, color='#4d9221')\n",
        "\tg.map_lower(corrfunc)\n",
        "  \n",
        "\tplt.style.use('seaborn-white')\n",
        "\tplt.style.use('seaborn-ticks')\n",
        "\tfilename = filename.replace('scaled.txt','')\n",
        "\tplt.savefig(f\"figs/Cross_telemetry_{filename}.png\", format='png', dpi=1200, facecolor='#f2f2f2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26BdKUcIQVQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = pd.read_csv('data/etalon_jitter_16Mar20_etalon.ccfSum-telemetry.csv')\n",
        "\n",
        "names = list(names.columns.values.tolist()) \n",
        "\n",
        "dataframe = pd.read_csv(f'data/Data12345scaled.txt', sep=\" \", header=None, names=names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW8fjaB5Ql2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary_analyze(dataframe, 'Data12345scaled.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrek2z8eZyxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}