{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVqXzSZSbegZ",
        "colab_type": "code",
        "outputId": "2af8ccd7-5f09-4e9d-a066-0ead3ef6231a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "%pip install bayesian-optimization"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.18.4)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.15.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkkNdR9mcAqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os, contextlib, sys\n",
        "#rom loaddata import loaddata\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import explained_variance_score, r2_score\n",
        "from sklearn.preprocessing import quantile_transform\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import KFold\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X2rWKP9cBRg",
        "colab_type": "code",
        "outputId": "a2a1963f-1687-42a7-ede5-f890c5c44203",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpiJwgJ6cfeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(seed=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XfV290XhlYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpu = torch.device(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jtatu44cfrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loaddata(directory):\n",
        "    feature_list = list()\n",
        "    train_data_list = list()\n",
        "    train_target_list = list()\n",
        "    for i,filename in enumerate(os.listdir(directory)):\n",
        "        raw_data = np.genfromtxt(f'{directory}/{filename}', delimiter=',', names=True)\n",
        "        data_matrix = np.genfromtxt(f'{directory}/{filename}', delimiter=',',skip_header=1)\n",
        "        \n",
        "        if i==0: \n",
        "            feature_list.append(raw_data.dtype.names)\n",
        "            train_data_list.append(data_matrix[:,[i for i in range(len(data_matrix[1])) if i!=1]])\n",
        "            train_target_list.append(data_matrix[:,1])\n",
        "        elif i==len(os.listdir(directory))-1:\n",
        "            test_data = data_matrix[:,[i for i in range(len(data_matrix[1])) if i!=1]]\n",
        "            test_target = data_matrix[:,1]\n",
        "        else:\n",
        "            train_data_list.append(data_matrix[:,[i for i in range(len(data_matrix[1])) if i!=1]])\n",
        "            train_target_list.append(data_matrix[:,1])\n",
        "    #train_data_list = np.vstack((train_data_list))\n",
        "    #train_target_list =  np.concatenate(train_target_list)\n",
        "    return train_data_list, train_target_list, test_data, test_target\n",
        "\n",
        "def get_explained(model, data, target):\n",
        "    model.eval()\n",
        "    pred = model(data).squeeze().detach().numpy()\n",
        "    target = target.detach().numpy()\n",
        "\n",
        "    return explained_variance_score(target, pred), r2_score(target, pred)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2HXwjUtfcWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_list, train_target_list, test_data, test_target = loaddata('data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t0ShoQTr9Zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_list = [quantile_transform(data,n_quantiles=100,copy=True) for data in train_data_list]\n",
        "test_data = quantile_transform(test_data,n_quantiles=100,copy=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl3oESXlfcEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_torch = [torch.from_numpy(data).unsqueeze(dim=1).float().to(gpu) for data in train_data_list]\n",
        "train_target_torch = [torch.from_numpy(data).unsqueeze(dim=1).float().to(gpu) for data in train_target_list]\n",
        "test_data_torch, test_target_torch = torch.from_numpy(np.array(test_data)).unsqueeze(dim=1).float().to(gpu), torch.from_numpy(np.array(test_target)).float().to(gpu)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVHhZdAyguoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, inputsize, layers, hiddensize, drop_out):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.inputsize = inputsize\n",
        "        self.hiddensize = hiddensize\n",
        "        self.layers = layers\n",
        "        self.drop_out = drop_out\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=self.inputsize, hidden_size = self.hiddensize, num_layers=layers, dropout=drop_out)\n",
        "        self.hidden2radial = nn.Linear(in_features=hiddensize, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.hidden2radial(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2fWRgYMhGUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDiK_PR_hGFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def NN_CrossValidation(hiddensize, layers, learning_rate, drop_out, data, targets):\n",
        "    estimator = LSTMTagger(27,layers=layers, hiddensize=hiddensize, drop_out=drop_out).to(gpu)\n",
        "    optimizer = torch.optim.Adam(estimator.parameters(), lr=learning_rate)\n",
        "    acc_list = list()\n",
        "    loss_list = list()\n",
        "    for i,valdata in enumerate(data):\n",
        "      traindata = data[:i] + data[i+1:]\n",
        "      traintarget = targets[:i] + targets[i+1:]\n",
        "      valtarget = targets[i]\n",
        "      n_epochs = 500\n",
        "      criterion = nn.MSELoss()\n",
        "      estimator.train()\n",
        "      for e in range(n_epochs):\n",
        "          epoch_evs = list()\n",
        "          epoch_losses = list()\n",
        "          for batch in range(len(traindata)):\n",
        "            estimator.zero_grad()\n",
        "            optimizer.zero_grad() \n",
        "            prediction = estimator(traindata[batch])\n",
        "            target = traintarget[batch]\n",
        "            # Calculating the loss function\n",
        "            loss = criterion(prediction.squeeze(dim=2),target)\n",
        "            epoch_losses.append(float(loss))\n",
        "            #evs = explained_variance_score(target.squeeze(dim=1).detach().cpu().numpy(),prediction.squeeze(dim=1).detach().cpu().numpy())\n",
        "            #epoch_evs.append(evs)\n",
        "            # Calculating the gradient  \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "          #print(e, np.mean(epoch_losses), np.mean(epoch_evs))\n",
        "      #loss_list.append(np.mean(epoch_losses))\n",
        "      with torch.no_grad():\n",
        "          estimator.eval()\n",
        "          train_prediction = estimator(valdata).squeeze(dim=1)\n",
        "          #loss_list.append( float(criterion((train_prediction),valtarget).detach().cpu()) )\n",
        "          acc_list.append( explained_variance_score(valtarget.cpu(), train_prediction.cpu()) )\n",
        "    return np.mean(acc_list)\n",
        "\n",
        "  \n",
        "\n",
        "def optimize_NN(data, targets, pars, n_iter=10):\n",
        "    def crossval_wrapper(hiddensize, layers, learning_rate, drop_out):\n",
        "        return NN_CrossValidation(hiddensize=int(hiddensize), layers=int(layers), learning_rate=learning_rate, drop_out=drop_out,\n",
        "                                            data=data, \n",
        "                                            targets=targets)\n",
        "\n",
        "    optimizer = BayesianOptimization(f=crossval_wrapper, \n",
        "                                     pbounds=pars, \n",
        "                                     random_state=27, \n",
        "                                     verbose=2)\n",
        "    optimizer.maximize(init_points=5, n_iter=n_iter)\n",
        "\n",
        "    return optimizer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDmBBoSpcf02",
        "colab_type": "code",
        "outputId": "cef7ce3c-a755-4948-ab46-0da19f137634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "parameters_BO = {\"hiddensize\": (400,500), \"learning_rate\": (0.0005,0.0001),\"layers\": (2,3),\"drop_out\": (0.1,0.3)}\n",
        "\n",
        "optimization_data = train_data_torch\n",
        "optimization_target = train_target_torch\n",
        "\n",
        "\n",
        "BO = optimize_NN(optimization_data,optimization_target,parameters_BO,n_iter=5)\n",
        "\n",
        "print(BO.max)\n",
        "\n",
        "params = BO.max['params']\n",
        "\n",
        "\n",
        "\n",
        "for key, val in params.items():\n",
        "    if key == 'hiddensize':\n",
        "        params[key] = int(val)\n",
        "    if key =='learning_rate':\n",
        "        params[key] = val\n",
        "    if key == 'layers':\n",
        "        params[key] = int(val)\n",
        "    if key == 'drop_out':\n",
        "        params[key] = val"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | drop_out  | hidden... |  layers   | learni... |\n",
            "-------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj9qa2fGeuEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e888a3d-c388-4dc2-9111-c1517e94b22f"
      },
      "source": [
        "model = LSTMTagger(27,layers=params['layers'], hiddensize=params['hiddensize'],drop_out=params['drop_out']).to(gpu)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=params['learning_rate'])\n",
        "\n",
        "for i,valdata in enumerate(train_data_torch):\n",
        "      traindata = train_data_torch[:i] + train_data_torch[i+1:]\n",
        "      traintarget = train_target_torch[:i] + train_target_torch[i+1:]\n",
        "      valtarget = train_target_torch[i]\n",
        "      n_epochs = 100\n",
        "      model.train()\n",
        "      for e in range(n_epochs):\n",
        "          epoch_losses = list()\n",
        "          epoch_evs = list()\n",
        "          for batch in range(len(train_data_list)):\n",
        "              model.zero_grad()\n",
        "              optimizer.zero_grad() \n",
        "              prediction = model(train_data_torch[batch])\n",
        "              target = train_target_torch[batch]\n",
        "              # Calculating the loss function\n",
        "              loss = criterion(prediction.squeeze(dim=2), target)\n",
        "              epoch_losses.append(float(loss))\n",
        "              evs = explained_variance_score(target.squeeze(dim=1).detach().cpu().numpy(),prediction.squeeze(dim=1).detach().cpu().numpy())\n",
        "              epoch_evs.append(evs)\n",
        "              # Calculating the gradient\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "          print(e, np.mean(epoch_losses),np.mean(epoch_evs))\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 14946.001953125 -2.5272369384765625e-05\n",
            "1 14944.240885416666 2.205371856689453e-05\n",
            "2 14943.120361328125 6.661812464396159e-05\n",
            "3 14942.050130208334 0.00012252728144327799\n",
            "4 14940.901936848959 0.00017380714416503906\n",
            "5 14939.565836588541 0.00023980935414632162\n",
            "6 14937.856119791666 0.0003323554992675781\n",
            "7 14935.738118489584 0.00045154492060343426\n",
            "8 14932.770263671875 0.0006246566772460938\n",
            "9 14928.677001953125 0.0008921225865681967\n",
            "10 14921.733072916666 0.0013882915178934734\n",
            "11 14909.126383463541 0.002381304899851481\n",
            "12 14874.511149088541 0.005809287230173747\n",
            "13 14761.803059895834 0.009718577067057291\n",
            "14 14656.06884765625 0.011144042015075684\n",
            "15 14565.116943359375 0.032308220863342285\n",
            "16 14513.157145182291 0.0467643141746521\n",
            "17 14488.232584635416 0.05155088504155477\n",
            "18 14444.85205078125 0.05176341533660889\n",
            "19 14403.533935546875 0.04854591687520345\n",
            "20 14367.799641927084 0.043873727321624756\n",
            "21 14329.11865234375 0.04879822333653768\n",
            "22 14285.498128255209 0.062047203381856285\n",
            "23 14249.445231119791 0.07094621658325195\n",
            "24 14217.667236328125 0.0703783631324768\n",
            "25 14710.32373046875 -0.01618492603302002\n",
            "26 14701.40966796875 -0.014302333196004232\n",
            "27 14775.488932291666 -0.03375011682510376\n",
            "28 14126.182861328125 0.007499237855275472\n",
            "29 14107.216796875 0.00914384921391805\n",
            "30 14083.972819010416 0.011299709479014078\n",
            "31 14057.005696614584 0.01258397102355957\n",
            "32 14122.052327473959 -0.0016505519549051921\n",
            "33 14096.861246744791 -0.0003542304039001465\n",
            "34 14073.260335286459 0.00029144684473673504\n",
            "35 14051.150472005209 0.0008250276247660319\n",
            "36 14031.270263671875 0.000900129477183024\n",
            "37 14013.124837239584 0.0008330742518107096\n",
            "38 13995.975992838541 0.0008603533109029134\n",
            "39 13979.75537109375 0.0009319782257080078\n",
            "40 13964.315673828125 0.000934600830078125\n",
            "41 13949.965006510416 0.0007051229476928711\n",
            "42 13936.034993489584 0.0007140040397644043\n",
            "43 13922.484049479166 0.0007645090421040853\n",
            "44 13909.5966796875 0.0007139841715494791\n",
            "45 13896.885904947916 0.0008020798365275065\n",
            "46 13884.661214192709 0.0007846554120381674\n",
            "47 13872.529866536459 0.0008469223976135254\n",
            "48 13860.846232096354 0.0007740259170532227\n",
            "49 13849.27001953125 0.0007734100023905436\n",
            "50 13837.852213541666 0.0009137392044067383\n",
            "51 13826.643636067709 0.0007917284965515137\n",
            "52 13815.636311848959 0.0007856488227844238\n",
            "53 13804.548502604166 0.0008561412493387858\n",
            "54 13793.739908854166 0.0008757909138997396\n",
            "55 13783.132242838541 0.0009707609812418619\n",
            "56 13772.436686197916 0.0009380578994750977\n",
            "57 13761.91357421875 0.0009764432907104492\n",
            "58 13751.385457356771 0.0010769367218017578\n",
            "59 13741.167805989584 0.0010311404863993328\n",
            "60 13730.889322916666 0.0010028282801310222\n",
            "61 13720.638224283854 0.0011011560757954915\n",
            "62 13710.608194986979 0.001172502835591634\n",
            "63 13700.42822265625 0.0010676980018615723\n",
            "64 13690.480021158854 0.0011521776517232258\n",
            "65 13680.437133789062 0.0011853774388631184\n",
            "66 13670.619303385416 0.0012160539627075195\n",
            "67 13660.788777669271 0.0011571844418843587\n",
            "68 13651.038248697916 0.0011053085327148438\n",
            "69 13641.199788411459 0.0013908743858337402\n",
            "70 13631.673217773438 0.0013843377431233723\n",
            "71 13621.781372070312 0.0013234416643778484\n",
            "72 13612.354410807291 0.001335740089416504\n",
            "73 13602.751302083334 0.0013953646024068196\n",
            "74 13593.218872070312 0.0013876159985860188\n",
            "75 13583.647054036459 0.0014832019805908203\n",
            "76 13574.147908528646 0.001523733139038086\n",
            "77 13564.623453776041 0.0014053980509440105\n",
            "78 13555.128377278646 0.0013782978057861328\n",
            "79 13545.634033203125 0.0014383196830749512\n",
            "80 13536.372721354166 0.0015570322672526042\n",
            "81 13526.796468098959 0.0014930566151936848\n",
            "82 13517.370686848959 0.0015913049379984538\n",
            "83 13507.990071614584 0.0015445152918497722\n",
            "84 13498.650349934896 0.0015647013982137044\n",
            "85 13489.315063476562 0.001585245132446289\n",
            "86 13480.154744466146 0.0016361474990844727\n",
            "87 13470.851725260416 0.001668413480122884\n",
            "88 13461.6708984375 0.0013869206110636394\n",
            "89 13452.574788411459 0.0015202760696411133\n",
            "90 13443.457194010416 0.0013511975606282551\n",
            "91 13434.321736653646 0.001495977242787679\n",
            "92 13425.170491536459 0.0014287829399108887\n",
            "93 13416.134358723959 0.0016816059748331706\n",
            "94 13407.192586263021 0.0017348726590474446\n",
            "95 13398.279256184896 0.001643816630045573\n",
            "96 13389.304646809896 0.001651306947072347\n",
            "97 13380.261555989584 0.0014508366584777832\n",
            "98 13371.483276367188 0.0016362468401590984\n",
            "99 13362.647094726562 0.001675109068552653\n",
            "0 13353.917928059896 0.001362621784210205\n",
            "1 13344.857991536459 0.0015515486399332683\n",
            "2 13336.124674479166 0.0017060637474060059\n",
            "3 13327.382039388021 0.0016950170199076335\n",
            "4 13318.642903645834 0.0016115903854370117\n",
            "5 13309.9150390625 0.001772145430246989\n",
            "6 13301.221313476562 0.0015617807706197102\n",
            "7 13292.52880859375 0.0016617774963378906\n",
            "8 13283.790974934896 0.0015902320543924968\n",
            "9 13275.150838216146 0.0017179250717163086\n",
            "10 13266.589558919271 0.0016248424847920735\n",
            "11 13257.83203125 0.0016518433888753254\n",
            "12 13249.145345052084 0.001605530579884847\n",
            "13 13240.591389973959 0.0015607674916585286\n",
            "14 13231.903605143229 0.0016743143399556477\n",
            "15 13223.381062825521 0.001821756362915039\n",
            "16 13214.900349934896 0.0016195972760518391\n",
            "17 13206.194010416666 0.001764833927154541\n",
            "18 13197.614664713541 0.00174407164255778\n",
            "19 13189.215372721354 0.0018633405367533367\n",
            "20 13180.744791666666 0.001662154992421468\n",
            "21 13172.338094075521 0.0016406774520874023\n",
            "22 13163.829182942709 0.001624464988708496\n",
            "23 13155.439737955729 0.0015808741251627605\n",
            "24 13146.969930013021 0.0017035206158955891\n",
            "25 13138.669881184896 0.001736144224802653\n",
            "26 13130.314086914062 0.0018772085507710774\n",
            "27 13121.897379557291 0.001686374346415202\n",
            "28 13113.637166341146 0.0017223358154296875\n",
            "29 13105.373250325521 0.001759966214497884\n",
            "30 13097.028645833334 0.001798411210378011\n",
            "31 13088.914184570312 0.0014241536458333333\n",
            "32 13080.490193684896 0.0017083287239074707\n",
            "33 13072.305053710938 0.0018038153648376465\n",
            "34 13064.153401692709 0.0017357269922892253\n",
            "35 13055.933349609375 0.002116243044535319\n",
            "36 13047.818400065104 0.0018253525098164876\n",
            "37 13039.622436523438 0.0015999277432759602\n",
            "38 13031.433675130209 0.0018471876780192058\n",
            "39 13023.413045247396 0.001768966515858968\n",
            "40 13015.416381835938 0.0017284552256266277\n",
            "41 13007.071899414062 0.0016209880510965984\n",
            "42 12999.160319010416 0.0017943382263183594\n",
            "43 12990.979125976562 0.0019494493802388508\n",
            "44 12982.951171875 0.0019868810971577964\n",
            "45 12974.968587239584 0.0019939939181009927\n",
            "46 12966.928426106771 0.0020114382108052573\n",
            "47 12958.816975911459 0.0018439094225565593\n",
            "48 12950.945027669271 0.0018689831097920735\n",
            "49 12942.854329427084 0.0016220410664876301\n",
            "50 12934.956746419271 0.0017194946606953938\n",
            "51 12926.808186848959 0.00185928742090861\n",
            "52 12918.789388020834 0.001871665318806966\n",
            "53 12910.706380208334 0.0018939971923828125\n",
            "54 12902.781453450521 0.0017836888631184895\n",
            "55 12894.858683268229 0.001753846804300944\n",
            "56 12886.751261393229 0.0021716952323913574\n",
            "57 12878.948689778646 0.0017458399136861165\n",
            "58 12870.955769856771 0.001847843329111735\n",
            "59 12862.969116210938 0.0022589564323425293\n",
            "60 12855.14306640625 0.0018450021743774414\n",
            "61 12847.337809244791 0.0019423762957255046\n",
            "62 12839.405924479166 0.0016832153002421062\n",
            "63 12831.696451822916 0.0021478931109110513\n",
            "64 12823.871948242188 0.0020399888356526694\n",
            "65 12815.984008789062 0.001941064993540446\n",
            "66 12808.053100585938 0.0019466280937194824\n",
            "67 12800.347981770834 0.0019124150276184082\n",
            "68 12792.648803710938 0.001989742120107015\n",
            "69 12784.759806315104 0.002299666404724121\n",
            "70 12777.112101236979 0.002075652281443278\n",
            "71 12769.489542643229 0.0020569562911987305\n",
            "72 12761.911539713541 0.0019175807634989421\n",
            "73 12754.091837565104 0.002086599667867025\n",
            "74 12746.365112304688 0.0019664367039998374\n",
            "75 12738.752807617188 0.0019379456837972004\n",
            "76 12731.21484375 0.0018502871195475261\n",
            "77 12723.620239257812 0.002023597558339437\n",
            "78 12715.676839192709 0.002361436684926351\n",
            "79 12708.33056640625 0.0019995768864949546\n",
            "80 12700.588623046875 0.0019223491350809734\n",
            "81 12693.136637369791 0.001959542433420817\n",
            "82 12685.512898763021 0.0021027525266011557\n",
            "83 12677.87353515625 0.002253572146097819\n",
            "84 12670.58203125 0.002170562744140625\n",
            "85 12663.078369140625 0.001973887284596761\n",
            "86 12655.384440104166 0.0023769338925679526\n",
            "87 12648.027425130209 0.002024988333384196\n",
            "88 12640.673217773438 0.0018447438875834148\n",
            "89 12633.093953450521 0.0020537575085957846\n",
            "90 12625.615844726562 0.002159555753072103\n",
            "91 12618.198486328125 0.002007285753885905\n",
            "92 12610.78515625 0.002619504928588867\n",
            "93 12603.521647135416 0.0022555788358052573\n",
            "94 12596.094441731771 0.0023889541625976562\n",
            "95 12588.496337890625 0.002116700013478597\n",
            "96 12581.049357096354 0.002011080582936605\n",
            "97 12573.623819986979 0.002134243647257487\n",
            "98 12566.260986328125 0.002069413661956787\n",
            "99 12558.715087890625 0.002102831999460856\n",
            "0 12551.124267578125 0.0020174384117126465\n",
            "1 12543.779012044271 0.001988232135772705\n",
            "2 12536.197102864584 0.0022186636924743652\n",
            "3 12528.556437174479 0.0022673606872558594\n",
            "4 12520.999796549479 0.002101421356201172\n",
            "5 12513.440022786459 0.0023039778073628745\n",
            "6 12506.062866210938 0.0023030241330464682\n",
            "7 12498.545491536459 0.002391556898752848\n",
            "8 12491.418375651041 0.002219220002492269\n",
            "9 12483.879923502604 0.002701441446940104\n",
            "10 12476.712890625 0.0021851460138956704\n",
            "11 12469.280802408854 0.0024939775466918945\n",
            "12 12462.008138020834 0.0025398929913838706\n",
            "13 12454.770304361979 0.0028054515520731607\n",
            "14 12447.567667643229 0.0026646852493286133\n",
            "15 12440.202718098959 0.0022149085998535156\n",
            "16 12432.919799804688 0.002474625905354818\n",
            "17 12425.511271158854 0.002596894900004069\n",
            "18 12418.437052408854 0.0026898781458536782\n",
            "19 12411.202880859375 0.002125402291615804\n",
            "20 12404.075805664062 0.002196927865346273\n",
            "21 12396.864379882812 0.0024114052454630532\n",
            "22 12389.736775716146 0.002680838108062744\n",
            "23 12382.659912109375 0.002225677172342936\n",
            "24 12375.461588541666 0.002586662769317627\n",
            "25 12368.475260416666 0.0022062460581461587\n",
            "26 12361.362630208334 0.0022626916567484536\n",
            "27 12354.247477213541 0.0025219122568766275\n",
            "28 12347.038167317709 0.0030788183212280273\n",
            "29 12339.995564778646 0.00273976723353068\n",
            "30 12332.813761393229 0.0026332338651021323\n",
            "31 12325.922159830729 0.002711057662963867\n",
            "32 12318.831909179688 0.0029125610987345376\n",
            "33 12311.704956054688 0.0024989843368530273\n",
            "34 12304.812540690104 0.0028012990951538086\n",
            "35 12297.4951171875 0.0025157531102498374\n",
            "36 12290.337076822916 0.00283968448638916\n",
            "37 12283.334025065104 0.0027277866999308267\n",
            "38 12276.076334635416 0.0026478171348571777\n",
            "39 12269.078369140625 0.0031266609827677407\n",
            "40 12262.162434895834 0.0026953816413879395\n",
            "41 12254.9375 0.0028716127077738443\n",
            "42 12247.872151692709 0.0028846263885498047\n",
            "43 12241.021199544271 0.0026186307271321616\n",
            "44 12233.825439453125 0.003654619057973226\n",
            "45 12226.646525065104 0.0029667019844055176\n",
            "46 12219.621297200521 0.0027979214986165366\n",
            "47 12212.701212565104 0.002979755401611328\n",
            "48 12205.491495768229 0.0030368963877360025\n",
            "49 12198.613159179688 0.003256837526957194\n",
            "50 12191.575398763021 0.0028145909309387207\n",
            "51 12184.604410807291 0.0029271841049194336\n",
            "52 12177.744262695312 0.0033962527910868325\n",
            "53 12170.809651692709 0.0033909479777018228\n",
            "54 12163.68994140625 0.003528118133544922\n",
            "55 12157.169270833334 0.003901799519856771\n",
            "56 12150.101725260416 0.0034800966580708823\n",
            "57 12143.420003255209 0.003929634888966878\n",
            "58 12136.415161132812 0.0032416979471842446\n",
            "59 12129.458780924479 0.0034168163935343423\n",
            "60 12122.645589192709 0.0037336349487304688\n",
            "61 12115.829549153646 0.004122575124104817\n",
            "62 12109.509969075521 0.003462990125020345\n",
            "63 12102.383097330729 0.003953774770100911\n",
            "64 12095.535522460938 0.003776729106903076\n",
            "65 12088.92578125 0.003217637538909912\n",
            "66 12082.150553385416 0.0037122766176859536\n",
            "67 12075.606323242188 0.005560338497161865\n",
            "68 12068.616821289062 0.004650433858235677\n",
            "69 12062.021525065104 0.0035609006881713867\n",
            "70 12055.257853190104 0.003700872262318929\n",
            "71 12048.552530924479 0.004038135210673015\n",
            "72 12041.904337565104 0.004046003023783366\n",
            "73 12035.162068684896 0.005185325940450032\n",
            "74 12028.668823242188 0.005379120508829753\n",
            "75 12022.17431640625 0.006449202696482341\n",
            "76 12015.386149088541 0.004453778266906738\n",
            "77 12008.802897135416 0.0043447017669677734\n",
            "78 12002.354410807291 0.004434625307718913\n",
            "79 11995.738566080729 0.004393140474955241\n",
            "80 11989.173014322916 0.004114290078481038\n",
            "81 11982.365519205729 0.005067209402720134\n",
            "82 11975.934651692709 0.005934536457061768\n",
            "83 11969.320922851562 0.006192704041798909\n",
            "84 11962.845296223959 0.0045616428057352705\n",
            "85 11956.404378255209 0.005336642265319824\n",
            "86 11949.960815429688 0.004757861296335856\n",
            "87 11943.403361002604 0.006848295529683431\n",
            "88 11936.953572591146 0.004231313864390056\n",
            "89 11930.4208984375 0.006642520427703857\n",
            "90 11924.053914388021 0.005034883817036946\n",
            "91 11917.632609049479 0.005479852358500163\n",
            "92 11911.302205403646 0.004885355631510417\n",
            "93 11905.009684244791 0.005732615788777669\n",
            "94 11898.357625325521 0.006932397683461507\n",
            "95 11891.781046549479 0.00528023640314738\n",
            "96 11885.770263671875 0.005053400993347168\n",
            "97 11879.250203450521 0.0056329766909281416\n",
            "98 11872.896769205729 0.006790101528167725\n",
            "99 11866.44677734375 0.005890905857086182\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZZQx2rOe19y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "time = test_data[:,0]\n",
        "data_plot = test_data_torch\n",
        "RV_pred = model(data_plot).squeeze(dim=2).detach().cpu().numpy()\n",
        "RV_true = test_target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2_MdOXINW2X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "e0bb3445-84db-47d3-8efc-6a9f8a2d0236"
      },
      "source": [
        "plt.figure(1)\n",
        "plt.plot(time,RV_pred,'k.')\n",
        "plt.plot(time,RV_true, 'r.')\n",
        "plt.show()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJjDus9qQ1Hv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}